diff --git a/.gitignore b/.gitignore
index 7508476..acc9eea 100644
--- a/.gitignore
+++ b/.gitignore
@@ -401,3 +401,10 @@ FodyWeavers.xsd
 # build
 build/*
 dist/*
+
+#jp dev
+.specstory
+.venv
+.vscode
+docs-dev/
+requirements-dev.txt
\ No newline at end of file
diff --git a/README.md b/README.md
index 0c32850..60dabb8 100644
--- a/README.md
+++ b/README.md
@@ -133,6 +133,38 @@ If you find this repo helpful, please cite the following papers:
 }
 ```
 
+## ğŸŒ¸ Japanese Text Support
+
+LLMLingua-JP extends the original LLMLingua with first-class support for Japanese text compression. It uses **fugashi + unidic-lite** for accurate Japanese tokenization and provides automatic language detection.
+
+### Key Features:
+- **Automatic Japanese Detection**: Detects Japanese text using Unicode character analysis
+- **Accurate Tokenization**: Uses fugashi with unidic-lite dictionary for proper Japanese word segmentation
+- **Preserved Sentence Structure**: Maintains original sentence order and punctuation
+- **Backward Compatibility**: Fully compatible with existing English functionality
+
+### Usage:
+```python
+from llmlingua import PromptCompressor
+
+compressor = PromptCompressor()
+
+# Automatic language detection
+result = compressor.compress_prompt(
+    context=["è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™ã€‚", "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã‚’æ´»ç”¨ã—ã¾ã™ã€‚"],
+    question="è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
+    rate=0.5,
+    lang="auto"  # Automatically detects Japanese
+)
+
+# Explicit Japanese processing
+result = compressor.compress_prompt(
+    context=["ç§ã¯å­¦ç”Ÿã§ã™ã€‚", "å¤§å­¦ã§å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚"],
+    rate=0.5,
+    lang="ja"  # Force Japanese processing
+)
+```
+
 ## ğŸ¯ Quick Start
 
 #### 1. **Installing LLMLingua:**
@@ -143,6 +175,12 @@ To get started with LLMLingua, simply install it using pip:
 pip install llmlingua
 ```
 
+**For Japanese text support**, install with additional dependencies:
+
+```bash
+pip install llmlingua[fugashi,unidic-lite]
+```
+
 #### 2. **Using LLMLingua Series Methods for Prompt Compression:**
 
 With **LLMLingua**, you can easily compress your prompts. Hereâ€™s how you can do it:
@@ -153,6 +191,16 @@ from llmlingua import PromptCompressor
 llm_lingua = PromptCompressor()
 compressed_prompt = llm_lingua.compress_prompt(prompt, instruction="", question="", target_token=200)
 
+# For Japanese text compression
+japanese_prompt = ["è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™ã€‚", "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã‚’æ´»ç”¨ã—ã¾ã™ã€‚"]
+compressed_jp = llm_lingua.compress_prompt(
+    context=japanese_prompt,
+    question="è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
+    rate=0.5,
+    lang="ja"  # or "auto" for automatic detection
+)
+```
+
 # > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\nLets think step step\nSam bought 1 boxes x00 oflters.\nHe bought 12 * 300ters in total\nSam then took 5 boxes 6ters0ters.\nHe sold these boxes for 5 *5\nAfterelling these  boxes there were 3030 highlighters remaining.\nThese form 330 / 3 = 110 groups of three pens.\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\nIn total, then, he earned $220 + $15 = $235.\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is 115',
 #  'origin_tokens': 2365,
 #  'compressed_tokens': 211,
diff --git a/examples/japanese_compression_demo.py b/examples/japanese_compression_demo.py
new file mode 100644
index 0000000..6ce8930
--- /dev/null
+++ b/examples/japanese_compression_demo.py
@@ -0,0 +1,160 @@
+#!/usr/bin/env python3
+"""
+Japanese Prompt Compression Demo
+
+This script demonstrates the Japanese text compression capabilities of LLMLingua-JP.
+"""
+
+import time
+from llmlingua import PromptCompressor
+
+
+def demo_japanese_compression():
+    """Demonstrate Japanese text compression."""
+    
+    print("ğŸŒ¸ LLMLingua-JP Japanese Compression Demo")
+    print("=" * 50)
+    
+    # Initialize compressor
+    print("Loading compressor...")
+    compressor = PromptCompressor(device_map="cpu")
+    
+    # Sample Japanese text
+    japanese_context = [
+        "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processingã€NLPï¼‰ã¯ã€",
+        "äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è‡ªç„¶è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹ä¸€é€£ã®æŠ€è¡“ã§ã‚ã‚Šã€",
+        "äººå·¥çŸ¥èƒ½ã¨è¨€èªå­¦ã®ä¸€åˆ†é‡ã§ã‚ã‚‹ã€‚",
+        "è‡ªç„¶è¨€èªå‡¦ç†ã®ç ”ç©¶ã¯ã€è¨ˆç®—è¨€èªå­¦ï¼ˆcomputational linguisticsï¼‰ã¨å¯†æ¥ã«é–¢é€£ã—ã¦ãŠã‚Šã€",
+        "ä¸¡è€…ã‚’åˆã‚ã›ã¦è¨ˆç®—è¨€èªå­¦ã¨å‘¼ã°ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚",
+        "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€éŸ³å£°èªè­˜ã€æ©Ÿæ¢°ç¿»è¨³ã€æƒ…å ±æŠ½å‡ºã€è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã€",
+        "ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã€æ„Ÿæƒ…åˆ†æãªã©ã€æ§˜ã€…ãªå¿œç”¨åˆ†é‡ãŒã‚ã‚‹ã€‚",
+        "è¿‘å¹´ã§ã¯ã€æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€è‡ªç„¶è¨€èªå‡¦ç†ã®æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã—ã¦ã„ã‚‹ã€‚",
+        "ç‰¹ã«ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ãŒå¤šãã®ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚"
+    ]
+    
+    question = "è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿãã®å¿œç”¨åˆ†é‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
+    
+    print(f"\nğŸ“ Original Text ({len(japanese_context)} sentences):")
+    for i, text in enumerate(japanese_context, 1):
+        print(f"  {i}. {text}")
+    
+    print(f"\nâ“ Question: {question}")
+    
+    # Test different compression rates
+    rates = [0.8, 0.6, 0.4, 0.2]
+    
+    for rate in rates:
+        print(f"\nğŸ”§ Compressing with rate {rate}...")
+        start_time = time.time()
+        
+        result = compressor.compress_prompt(
+            context=japanese_context,
+            question=question,
+            rate=rate,
+            lang="ja"
+        )
+        
+        end_time = time.time()
+        
+        print(f"â±ï¸  Compression time: {end_time - start_time:.2f}s")
+        print(f"ğŸ“Š Original tokens: {result['origin_tokens']}")
+        print(f"ğŸ“Š Compressed tokens: {result['compressed_tokens']}")
+        print(f"ğŸ“Š Compression ratio: {result['ratio']}")
+        print(f"ğŸ’° Estimated savings: {result['saving']}")
+        
+        print(f"\nğŸ“„ Compressed text:")
+        print(f"  {result['compressed_prompt'][:200]}...")
+        print("-" * 50)
+
+
+def demo_auto_detection():
+    """Demonstrate automatic language detection."""
+    
+    print("\nğŸŒ Language Auto-Detection Demo")
+    print("=" * 50)
+    
+    compressor = PromptCompressor(device_map="cpu")
+    
+    # Mixed language text
+    mixed_context = [
+        "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processingï¼‰ã¯ã€",
+        "äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
+        "Machine learning and deep learning are used.",
+        "éŸ³å£°èªè­˜ã‚„æ©Ÿæ¢°ç¿»è¨³ãªã©ã®å¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚"
+    ]
+    
+    print("ğŸ“ Mixed Language Text:")
+    for text in mixed_context:
+        print(f"  {text}")
+    
+    print("\nğŸ” Testing auto-detection...")
+    result = compressor.compress_prompt(
+        context=mixed_context,
+        rate=0.5,
+        lang="auto"
+    )
+    
+    print(f"ğŸ“Š Result: {result['compressed_tokens']} tokens (from {result['origin_tokens']})")
+    print(f"ğŸ“„ Compressed: {result['compressed_prompt'][:150]}...")
+
+
+def demo_performance_comparison():
+    """Compare performance with and without Japanese processing."""
+    
+    print("\nâš¡ Performance Comparison")
+    print("=" * 50)
+    
+    compressor = PromptCompressor(device_map="cpu")
+    
+    # English text
+    english_context = [
+        "Natural Language Processing (NLP) is a field of artificial intelligence.",
+        "It focuses on the interaction between computers and human language.",
+        "Machine learning and deep learning techniques are widely used.",
+        "Applications include speech recognition, machine translation, and text summarization."
+    ]
+    
+    # Japanese text
+    japanese_context = [
+        "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™ã€‚",
+        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨äººé–“ã®è¨€èªã®ç›¸äº’ä½œç”¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚",
+        "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã®æŠ€è¡“ãŒåºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
+        "å¿œç”¨åˆ†é‡ã«ã¯éŸ³å£°èªè­˜ã€æ©Ÿæ¢°ç¿»è¨³ã€ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ãªã©ãŒã‚ã‚Šã¾ã™ã€‚"
+    ]
+    
+    print("ğŸ”¤ English Text Processing:")
+    start_time = time.time()
+    en_result = compressor.compress_prompt(context=english_context, rate=0.5)
+    en_time = time.time() - start_time
+    
+    print(f"  Time: {en_time:.2f}s")
+    print(f"  Tokens: {en_result['compressed_tokens']}/{en_result['origin_tokens']}")
+    
+    print("\nğŸ‡¯ğŸ‡µ Japanese Text Processing:")
+    start_time = time.time()
+    jp_result = compressor.compress_prompt(context=japanese_context, rate=0.5, lang="ja")
+    jp_time = time.time() - start_time
+    
+    print(f"  Time: {jp_time:.2f}s")
+    print(f"  Tokens: {jp_result['compressed_tokens']}/{jp_result['origin_tokens']}")
+    
+    print(f"\nğŸ“ˆ Performance difference: {jp_time - en_time:.2f}s")
+
+
+if __name__ == "__main__":
+    try:
+        demo_japanese_compression()
+        demo_auto_detection()
+        demo_performance_comparison()
+        
+        print("\nâœ… Demo completed successfully!")
+        print("\nğŸ’¡ Tips:")
+        print("  - Use lang='auto' for automatic language detection")
+        print("  - Use lang='ja' for explicit Japanese processing")
+        print("  - Adjust rate parameter to control compression level")
+        print("  - Japanese tokenization preserves sentence structure")
+        
+    except Exception as e:
+        print(f"âŒ Error during demo: {e}")
+        print("Make sure you have installed the required dependencies:")
+        print("  pip install fugashi unidic-lite") 
\ No newline at end of file
diff --git a/llmlingua/prompt_compressor.py b/llmlingua/prompt_compressor.py
index 7af7920..be40b81 100644
--- a/llmlingua/prompt_compressor.py
+++ b/llmlingua/prompt_compressor.py
@@ -463,6 +463,7 @@ class PromptCompressor:
         drop_consecutive: bool = False,
         chunk_end_tokens: List[str] = [".", "\n"],
         strict_preserve_uncompressed: bool = True,
+        lang: str = "auto",
     ):
         """
         Compresses the given context.
@@ -559,6 +560,24 @@ class PromptCompressor:
             context = [" "]
         if isinstance(context, str):
             context = [context]
+
+        # Japanese language detection and tokenization
+        if lang == "auto":
+            # Auto-detect Japanese text
+            from .tokenizer_jp import is_japanese_text
+
+            all_text = "\n\n".join([instruction] + context + [question]).strip()
+            lang = "ja" if is_japanese_text(all_text) else "en"
+        elif lang == "ja":
+            # Apply Japanese tokenization
+            from .tokenizer_jp import tokenize_jp
+
+            context = [tokenize_jp(text) for text in context]
+            if instruction:
+                instruction = tokenize_jp(instruction)
+            if question:
+                question = tokenize_jp(question)
+
         assert not (
             rank_method == "longllmlingua" and not question
         ), "In the LongLLMLingua, it is necessary to set a question."
@@ -2270,9 +2289,11 @@ class PromptCompressor:
                 words.append(token)
                 word_probs.append(
                     [
-                        1.0
-                        if force_reserve_digit and bool(re.search(r"\d", token))
-                        else prob
+                        (
+                            1.0
+                            if force_reserve_digit and bool(re.search(r"\d", token))
+                            else prob
+                        )
                     ]
                 )
                 word_probs_no_force.append([prob_no_force])
diff --git a/llmlingua/tokenizer_jp.py b/llmlingua/tokenizer_jp.py
new file mode 100644
index 0000000..cff6485
--- /dev/null
+++ b/llmlingua/tokenizer_jp.py
@@ -0,0 +1,117 @@
+"""
+Japanese tokenizer for LLMLingua using fugashi + unidic-lite.
+Provides tokenization that preserves sentence structure and punctuation.
+"""
+
+import re
+from typing import List, Tuple
+
+try:
+    import fugashi
+    import unidic_lite
+except ImportError:
+    raise ImportError(
+        "Japanese tokenization requires fugashi and unidic-lite. "
+        "Install with: pip install fugashi unidic-lite"
+    )
+
+
+class JapaneseTokenizer:
+    """Japanese text tokenizer using fugashi with unidic-lite dictionary."""
+
+    def __init__(self):
+        """Initialize the Japanese tokenizer."""
+        self.tagger = fugashi.Tagger()
+
+    def tokenize(self, text: str) -> List[str]:
+        """
+        Tokenize Japanese text into words.
+
+        Args:
+            text: Input Japanese text
+
+        Returns:
+            List of tokenized words
+        """
+        if not text or not text.strip():
+            return []
+
+        # Parse with fugashi
+        words = self.tagger(text)
+
+        # Extract surface forms and filter empty tokens
+        tokens = [word.surface for word in words if word.surface.strip()]
+
+        return tokens
+
+    def tokenize_with_pos(self, text: str) -> List[Tuple[str, str]]:
+        """
+        Tokenize Japanese text with part-of-speech information.
+
+        Args:
+            text: Input Japanese text
+
+        Returns:
+            List of (token, pos) tuples
+        """
+        if not text or not text.strip():
+            return []
+
+        words = self.tagger(text)
+        tokens_with_pos = [
+            (word.surface, word.pos) for word in words if word.surface.strip()
+        ]
+
+        return tokens_with_pos
+
+
+def tokenize_jp(text: str, preserve_punctuation: bool = True) -> str:
+    """
+    Tokenize Japanese text and return space-separated string.
+
+    Args:
+        text: Input Japanese text
+        preserve_punctuation: Whether to preserve punctuation marks
+
+    Returns:
+        Space-separated tokenized text
+    """
+    tokenizer = JapaneseTokenizer()
+    tokens = tokenizer.tokenize(text)
+
+    if not preserve_punctuation:
+        # Remove punctuation tokens
+        tokens = [token for token in tokens if not re.match(r"^[^\w\s]+$", token)]
+
+    return " ".join(tokens)
+
+
+def is_japanese_text(text: str, threshold: float = 0.3) -> bool:
+    """
+    Detect if text contains Japanese characters.
+
+    Args:
+        text: Input text to check
+        threshold: Minimum ratio of Japanese characters to consider as Japanese
+
+    Returns:
+        True if text is likely Japanese
+    """
+    if not text:
+        return False
+
+    # Japanese character ranges
+    hiragana = "\u3040-\u309f"
+    katakana = "\u30a0-\u30ff"
+    kanji = "\u4e00-\u9faf"
+    jp_chars = f"[{hiragana}{katakana}{kanji}]"
+
+    # Count Japanese characters
+    jp_char_count = len(re.findall(jp_chars, text))
+    total_chars = len(text.strip())
+
+    if total_chars == 0:
+        return False
+
+    ratio = jp_char_count / total_chars
+    return ratio >= threshold
diff --git a/llmlingua/utils.py b/llmlingua/utils.py
index a08f615..897ca6b 100644
--- a/llmlingua/utils.py
+++ b/llmlingua/utils.py
@@ -79,9 +79,11 @@ def seed_everything(seed: int):
 
 
 def is_begin_of_new_word(token, model_name, force_tokens, token_map):
-    if "bert-base-multilingual-cased" in model_name \
-            or "tinybert" in model_name.lower() \
-            or "mobilebert" in model_name.lower():
+    if (
+        "bert-base-multilingual-cased" in model_name
+        or "tinybert" in model_name.lower()
+        or "mobilebert" in model_name.lower()
+    ):
         if token.lstrip("##") in force_tokens or token.lstrip("##") in set(
             token_map.values()
         ):
@@ -106,9 +108,11 @@ def replace_added_token(token, token_map):
 
 
 def get_pure_token(token, model_name):
-    if "bert-base-multilingual-cased" in model_name \
-            or "tinybert" in model_name.lower() \
-            or "mobilebert" in model_name.lower():
+    if (
+        "bert-base-multilingual-cased" in model_name
+        or "tinybert" in model_name.lower()
+        or "mobilebert" in model_name.lower()
+    ):
         return token.lstrip("##")
     elif "xlm-roberta-large" in model_name:
         return token.lstrip("â–")
diff --git a/setup.py b/setup.py
index ae0f4fb..d810261 100644
--- a/setup.py
+++ b/setup.py
@@ -29,6 +29,8 @@ INSTALL_REQUIRES = [
     "tiktoken",
     "nltk",
     "numpy",
+    "fugashi>=1.2.0",
+    "unidic-lite>=1.0.8",
 ]
 QUANLITY_REQUIRES = [
     "black==21.4b0",
diff --git a/tests/test_integration_jp.py b/tests/test_integration_jp.py
new file mode 100644
index 0000000..f10fd39
--- /dev/null
+++ b/tests/test_integration_jp.py
@@ -0,0 +1,153 @@
+"""
+Integration tests for Japanese prompt compression.
+"""
+
+import pytest
+from llmlingua import PromptCompressor
+
+
+class TestJapaneseIntegration:
+    """Integration tests for Japanese prompt compression."""
+    
+    @pytest.fixture
+    def compressor(self):
+        """Create a compressor instance for testing."""
+        return PromptCompressor(device_map="cpu")
+    
+    def test_japanese_compression_basic(self, compressor):
+        """Test basic Japanese text compression."""
+        context = [
+            "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processingï¼‰ã¯ã€",
+            "äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è‡ªç„¶è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹ä¸€é€£ã®æŠ€è¡“ã§ã™ã€‚",
+            "äººå·¥çŸ¥èƒ½ã¨è¨€èªå­¦ã®ä¸€åˆ†é‡ã¨ã—ã¦ä½ç½®ã¥ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚"
+        ]
+        
+        result = compressor.compress_prompt(
+            context=context,
+            rate=0.5,
+            lang="ja"
+        )
+        
+        assert "compressed_prompt" in result
+        assert result["compressed_tokens"] <= result["origin_tokens"]
+        assert result["compressed_tokens"] > 0
+        
+        # Should preserve some Japanese content
+        compressed = result["compressed_prompt"]
+        assert any(ord(char) > 127 for char in compressed)
+    
+    def test_japanese_auto_detection(self, compressor):
+        """Test automatic Japanese language detection."""
+        jp_context = ["æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚", "æ©Ÿæ¢°å­¦ç¿’ã‚’æ´»ç”¨ã—ã¾ã™ã€‚"]
+        en_context = ["This is English text.", "Machine learning is used."]
+        
+        # Japanese should be detected
+        jp_result = compressor.compress_prompt(
+            context=jp_context,
+            rate=0.5,
+            lang="auto"
+        )
+        
+        # English should not trigger Japanese processing
+        en_result = compressor.compress_prompt(
+            context=en_context,
+            rate=0.5,
+            lang="auto"
+        )
+        
+        assert jp_result["compressed_tokens"] <= jp_result["origin_tokens"]
+        assert en_result["compressed_tokens"] <= en_result["origin_tokens"]
+    
+    def test_japanese_with_question(self, compressor):
+        """Test Japanese compression with question."""
+        context = [
+            "æ±äº¬ã¯æ—¥æœ¬ã®é¦–éƒ½ã§ã™ã€‚",
+            "äººå£ã¯ç´„1400ä¸‡äººã§ã™ã€‚",
+            "å¤šãã®ä¼æ¥­ã®æœ¬ç¤¾ãŒã‚ã‚Šã¾ã™ã€‚"
+        ]
+        question = "æ±äº¬ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
+        
+        result = compressor.compress_prompt(
+            context=context,
+            question=question,
+            rate=0.5,
+            lang="ja"
+        )
+        
+        assert "compressed_prompt" in result
+        assert result["compressed_tokens"] <= result["origin_tokens"]
+    
+    def test_japanese_compression_ratio_control(self, compressor):
+        """Test Japanese compression ratio control."""
+        context = [
+            "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
+            "æ©Ÿæ¢°å­¦ç¿’ã€ç‰¹ã«æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚",
+            "éŸ³å£°èªè­˜ã€æ©Ÿæ¢°ç¿»è¨³ã€æƒ…å ±æŠ½å‡ºãªã©ã€æ§˜ã€…ãªå¿œç”¨åˆ†é‡ãŒã‚ã‚Šã¾ã™ã€‚"
+        ]
+        
+        # High compression rate
+        high_result = compressor.compress_prompt(
+            context=context,
+            rate=0.3,
+            lang="ja"
+        )
+        
+        # Low compression rate
+        low_result = compressor.compress_prompt(
+            context=context,
+            rate=0.8,
+            lang="ja"
+        )
+        
+        # High compression should result in fewer tokens
+        high_ratio = high_result["compressed_tokens"] / high_result["origin_tokens"]
+        low_ratio = low_result["compressed_tokens"] / low_result["origin_tokens"]
+        
+        assert high_ratio <= low_ratio
+    
+    def test_japanese_backward_compatibility(self, compressor):
+        """Test that Japanese support doesn't break English functionality."""
+        en_context = ["This is English text for testing.", "It should work normally."]
+        
+        # Should work with default settings (no lang parameter)
+        result = compressor.compress_prompt(
+            context=en_context,
+            rate=0.5
+        )
+        
+        assert "compressed_prompt" in result
+        assert result["compressed_tokens"] <= result["origin_tokens"]
+        
+        # Should work with explicit English
+        result_explicit = compressor.compress_prompt(
+            context=en_context,
+            rate=0.5,
+            lang="auto"  # Should detect as English
+        )
+        
+        assert "compressed_prompt" in result_explicit
+        assert result_explicit["compressed_tokens"] <= result_explicit["origin_tokens"]
+
+
+@pytest.mark.benchmark
+class TestJapanesePerformance:
+    """Performance tests for Japanese compression."""
+    
+    def test_japanese_tokenization_speed(self, benchmark):
+        """Benchmark Japanese tokenization speed."""
+        from llmlingua.tokenizer_jp import tokenize_jp
+        
+        text = "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã‚ã‚Šã€æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã‚’æ´»ç”¨ã—ã¦äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚"
+        
+        result = benchmark(tokenize_jp, text)
+        assert isinstance(result, str)
+        assert len(result) > 0
+    
+    def test_japanese_detection_speed(self, benchmark):
+        """Benchmark Japanese text detection speed."""
+        from llmlingua.tokenizer_jp import is_japanese_text
+        
+        text = "Hello ä¸–ç•Œï¼This is a mixed text with Japanese characters."
+        
+        result = benchmark(is_japanese_text, text)
+        assert isinstance(result, bool) 
\ No newline at end of file
diff --git a/tests/test_prompt_compressor_jp.py b/tests/test_prompt_compressor_jp.py
new file mode 100644
index 0000000..3d36add
--- /dev/null
+++ b/tests/test_prompt_compressor_jp.py
@@ -0,0 +1,154 @@
+"""
+Tests for Japanese prompt compression functionality.
+"""
+
+import pytest
+
+from llmlingua import PromptCompressor
+
+
+class TestJapanesePromptCompression:
+    """Test cases for Japanese prompt compression."""
+
+    def test_japanese_detection_auto(self):
+        """Test automatic Japanese text detection."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        # Japanese text
+        jp_context = [
+            "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™ã€‚",
+            "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã‚’æ´»ç”¨ã—ã¾ã™ã€‚",
+        ]
+        jp_question = "è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
+
+        result = compressor.compress_prompt(
+            context=jp_context, question=jp_question, rate=0.5, lang="auto"
+        )
+
+        assert "compressed_prompt" in result
+        assert "origin_tokens" in result
+        assert "compressed_tokens" in result
+        assert result["compressed_tokens"] <= result["origin_tokens"]
+
+    def test_japanese_explicit_lang(self):
+        """Test explicit Japanese language setting."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        jp_context = ["ç§ã¯å­¦ç”Ÿã§ã™ã€‚", "å¤§å­¦ã§å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚"]
+        jp_question = "ã‚ãªãŸã¯ä½•ã‚’ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ"
+
+        result = compressor.compress_prompt(
+            context=jp_context, question=jp_question, rate=0.5, lang="ja"
+        )
+
+        assert "compressed_prompt" in result
+        assert result["compressed_tokens"] <= result["origin_tokens"]
+
+    def test_mixed_japanese_english(self):
+        """Test mixed Japanese and English text."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        mixed_context = [
+            "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processingï¼‰ã¯ã€",
+            "äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
+            "Machine learning and deep learning are used.",
+        ]
+        mixed_question = "What is NLP?"
+
+        result = compressor.compress_prompt(
+            context=mixed_context, question=mixed_question, rate=0.5, lang="auto"
+        )
+
+        assert "compressed_prompt" in result
+        assert result["compressed_tokens"] <= result["origin_tokens"]
+
+    def test_japanese_tokenization_preservation(self):
+        """Test that Japanese tokenization preserves meaning."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        jp_text = "ç¾ã—ã„æ¡œã®èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚"
+
+        result = compressor.compress_prompt(
+            context=[jp_text], rate=0.8, lang="ja"  # High rate to preserve more content
+        )
+
+        compressed = result["compressed_prompt"]
+        # Should contain some Japanese characters
+        assert any(ord(char) > 127 for char in compressed)
+
+    def test_japanese_compression_ratio(self):
+        """Test Japanese compression ratio."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        jp_context = [
+            "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è‡ªç„¶è¨€èªã‚’",
+            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹ä¸€é€£ã®æŠ€è¡“ã§ã‚ã‚Šã€",
+            "äººå·¥çŸ¥èƒ½ã¨è¨€èªå­¦ã®ä¸€åˆ†é‡ã§ã‚ã‚‹ã€‚",
+            "æ©Ÿæ¢°å­¦ç¿’ã€ç‰¹ã«æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€",
+            "è‡ªç„¶è¨€èªå‡¦ç†ã®æ€§èƒ½ã¯å¤§å¹…ã«å‘ä¸Šã—ãŸã€‚",
+        ]
+
+        result = compressor.compress_prompt(context=jp_context, rate=0.5, lang="ja")
+
+        # Check compression ratio
+        ratio = result["compressed_tokens"] / result["origin_tokens"]
+        assert ratio <= 0.6  # Should be compressed (allow some tolerance)
+
+    def test_japanese_with_instruction(self):
+        """Test Japanese compression with instruction."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        instruction = "ä»¥ä¸‹ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š"
+        jp_context = ["ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€‚", "å…¬åœ’ã§æ•£æ­©ã‚’ã—ã¾ã—ãŸã€‚"]
+
+        result = compressor.compress_prompt(
+            context=jp_context, instruction=instruction, rate=0.5, lang="ja"
+        )
+
+        assert "compressed_prompt" in result
+        assert result["compressed_tokens"] <= result["origin_tokens"]
+
+
+@pytest.mark.integration
+class TestJapaneseCompressionIntegration:
+    """Integration tests for Japanese compression."""
+
+    def test_long_japanese_text(self):
+        """Test compression of longer Japanese text."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        long_jp_text = [
+            "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆã—ãœã‚“ã’ã‚“ã”ã—ã‚‡ã‚Šã€è‹±èª: natural language processingã€ç•¥ç§°: NLPï¼‰ã¯ã€",
+            "äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è‡ªç„¶è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹ä¸€é€£ã®æŠ€è¡“ã§ã‚ã‚Šã€",
+            "äººå·¥çŸ¥èƒ½ã¨è¨€èªå­¦ã®ä¸€åˆ†é‡ã§ã‚ã‚‹ã€‚",
+            "è‡ªç„¶è¨€èªå‡¦ç†ã®ç ”ç©¶ã¯ã€è¨ˆç®—è¨€èªå­¦ï¼ˆcomputational linguisticsï¼‰ã¨å¯†æ¥ã«é–¢é€£ã—ã¦ãŠã‚Šã€",
+            "ä¸¡è€…ã‚’åˆã‚ã›ã¦è¨ˆç®—è¨€èªå­¦ã¨å‘¼ã°ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚",
+            "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€éŸ³å£°èªè­˜ã€æ©Ÿæ¢°ç¿»è¨³ã€æƒ…å ±æŠ½å‡ºã€è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã€",
+            "ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã€æ„Ÿæƒ…åˆ†æãªã©ã€æ§˜ã€…ãªå¿œç”¨åˆ†é‡ãŒã‚ã‚‹ã€‚",
+        ]
+
+        result = compressor.compress_prompt(
+            context=long_jp_text, rate=0.3, lang="ja"  # More aggressive compression
+        )
+
+        assert result["compressed_tokens"] < result["origin_tokens"]
+        assert result["compressed_tokens"] / result["origin_tokens"] <= 0.4
+
+    def test_japanese_special_characters(self):
+        """Test handling of Japanese special characters."""
+        compressor = PromptCompressor(device_map="cpu")
+
+        special_chars_text = [
+            "ã€Œå¼•ç”¨ã€ã‚„ï¼ˆæ‹¬å¼§ï¼‰ãªã©ã€æ§˜ã€…ãªè¨˜å·ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã€‚",
+            "ã‚«ã‚¿ã‚«ãƒŠï¼šã‚¢ã‚¤ã‚¦ã‚¨ã‚ªã€ã²ã‚‰ãŒãªï¼šã‚ã„ã†ãˆãŠ",
+            "æ¼¢å­—ï¼šè‡ªç„¶è¨€èªå‡¦ç†ã€æ•°å­—ï¼š12345",
+        ]
+
+        result = compressor.compress_prompt(
+            context=special_chars_text, rate=0.5, lang="ja"
+        )
+
+        assert "compressed_prompt" in result
+        # Should preserve some special characters
+        compressed = result["compressed_prompt"]
+        assert len(compressed) > 0
diff --git a/tests/test_tokenizer_jp.py b/tests/test_tokenizer_jp.py
new file mode 100644
index 0000000..cb3890b
--- /dev/null
+++ b/tests/test_tokenizer_jp.py
@@ -0,0 +1,161 @@
+"""
+Tests for Japanese tokenizer functionality.
+"""
+
+import pytest
+
+from llmlingua.tokenizer_jp import JapaneseTokenizer, is_japanese_text, tokenize_jp
+
+
+class TestJapaneseTokenizer:
+    """Test cases for JapaneseTokenizer class."""
+
+    def test_tokenizer_initialization(self):
+        """Test tokenizer can be initialized."""
+        tokenizer = JapaneseTokenizer()
+        assert tokenizer is not None
+        assert hasattr(tokenizer, "tagger")
+
+    def test_tokenize_basic_japanese(self):
+        """Test basic Japanese tokenization."""
+        tokenizer = JapaneseTokenizer()
+        text = "ç§ã¯å­¦ç”Ÿã§ã™ã€‚"
+        tokens = tokenizer.tokenize(text)
+
+        assert isinstance(tokens, list)
+        assert len(tokens) > 0
+        assert all(isinstance(token, str) for token in tokens)
+
+    def test_tokenize_with_pos(self):
+        """Test tokenization with part-of-speech information."""
+        tokenizer = JapaneseTokenizer()
+        text = "ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚"
+        tokens_with_pos = tokenizer.tokenize_with_pos(text)
+
+        assert isinstance(tokens_with_pos, list)
+        assert len(tokens_with_pos) > 0
+        assert all(
+            isinstance(item, tuple) and len(item) == 2 for item in tokens_with_pos
+        )
+
+    def test_empty_text(self):
+        """Test handling of empty text."""
+        tokenizer = JapaneseTokenizer()
+
+        assert tokenizer.tokenize("") == []
+        assert tokenizer.tokenize("   ") == []
+        assert tokenizer.tokenize_with_pos("") == []
+        assert tokenizer.tokenize_with_pos("   ") == []
+
+
+class TestTokenizeJp:
+    """Test cases for tokenize_jp function."""
+
+    def test_basic_tokenization(self):
+        """Test basic tokenize_jp functionality."""
+        text = "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚"
+        result = tokenize_jp(text)
+
+        assert isinstance(result, str)
+        assert len(result) > 0
+        assert " " in result  # Should be space-separated
+
+    def test_preserve_punctuation(self):
+        """Test punctuation preservation."""
+        text = "ã“ã‚“ã«ã¡ã¯ï¼å…ƒæ°—ã§ã™ã‹ï¼Ÿ"
+
+        # With punctuation
+        result_with = tokenize_jp(text, preserve_punctuation=True)
+        assert "ï¼" in result_with or "?" in result_with
+
+        # Without punctuation
+        result_without = tokenize_jp(text, preserve_punctuation=False)
+        assert "ï¼" not in result_without and "?" not in result_without
+
+    def test_mixed_text(self):
+        """Test mixed Japanese and English text."""
+        text = "Hello ä¸–ç•Œï¼This is a test."
+        result = tokenize_jp(text)
+
+        assert isinstance(result, str)
+        assert len(result) > 0
+
+    def test_empty_input(self):
+        """Test empty input handling."""
+        assert tokenize_jp("") == ""
+        assert tokenize_jp("   ") == ""
+
+
+class TestIsJapaneseText:
+    """Test cases for is_japanese_text function."""
+
+    def test_pure_japanese(self):
+        """Test pure Japanese text detection."""
+        text = "æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚"
+        assert is_japanese_text(text) is True
+
+    def test_mixed_text(self):
+        """Test mixed text detection."""
+        text = "Hello ä¸–ç•Œï¼This is a test."
+        # This text has 2 Japanese chars out of 24 total = 0.083 ratio
+        # With default threshold 0.3, this should be False
+        assert is_japanese_text(text) is False
+
+    def test_english_only(self):
+        """Test English-only text."""
+        text = "This is English text only."
+        assert is_japanese_text(text) is False
+
+    def test_empty_text(self):
+        """Test empty text handling."""
+        assert is_japanese_text("") is False
+        assert is_japanese_text("   ") is False
+
+    def test_custom_threshold(self):
+        """Test custom threshold setting."""
+        text = "Hello ä¸–ç•Œ"  # 2 Japanese chars, 8 total chars = 0.25 ratio
+
+        # Default threshold (0.3) should return False
+        assert is_japanese_text(text) is False
+
+        # Lower threshold should return True
+        assert is_japanese_text(text, threshold=0.2) is True
+
+    def test_hiragana_katakana_kanji(self):
+        """Test different Japanese character types."""
+        hiragana = "ã‚ã„ã†ãˆãŠ"
+        katakana = "ã‚¢ã‚¤ã‚¦ã‚¨ã‚ª"
+        kanji = "æ¼¢å­—"
+
+        assert is_japanese_text(hiragana) is True
+        assert is_japanese_text(katakana) is True
+        assert is_japanese_text(kanji) is True
+
+
+@pytest.mark.integration
+class TestTokenizerIntegration:
+    """Integration tests for tokenizer."""
+
+    def test_long_text(self):
+        """Test tokenization of longer text."""
+        text = """
+        è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆã—ãœã‚“ã’ã‚“ã”ã—ã‚‡ã‚Šã€è‹±èª: natural language processingã€ç•¥ç§°: NLPï¼‰ã¯ã€
+        äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è‡ªç„¶è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹ä¸€é€£ã®æŠ€è¡“ã§ã‚ã‚Šã€
+        äººå·¥çŸ¥èƒ½ã¨è¨€èªå­¦ã®ä¸€åˆ†é‡ã§ã‚ã‚‹ã€‚
+        """
+
+        result = tokenize_jp(text)
+        assert isinstance(result, str)
+        assert len(result) > 0
+
+        # Should preserve sentence structure
+        tokens = result.split()
+        assert len(tokens) > 10  # Should have multiple tokens
+
+    def test_special_characters(self):
+        """Test handling of special characters."""
+        text = "ã€Œå¼•ç”¨ã€ã‚„ï¼ˆæ‹¬å¼§ï¼‰ãªã©ã€æ§˜ã€…ãªè¨˜å·ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã€‚"
+        result = tokenize_jp(text)
+
+        assert isinstance(result, str)
+        assert len(result) > 0
