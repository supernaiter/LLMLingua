#!/usr/bin/env python3
"""
Japanese Prompt Compression Demo

This script demonstrates the Japanese text compression capabilities of LLMLingua-JP.
"""

import time
from llmlingua import PromptCompressor


def demo_japanese_compression():
    """Demonstrate Japanese text compression."""
    
    print("ğŸŒ¸ LLMLingua-JP Japanese Compression Demo")
    print("=" * 50)
    
    # Initialize compressor
    print("Loading compressor...")
    compressor = PromptCompressor(device_map="cpu")
    
    # Sample Japanese text
    japanese_context = [
        "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processingã€NLPï¼‰ã¯ã€",
        "äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è‡ªç„¶è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹ä¸€é€£ã®æŠ€è¡“ã§ã‚ã‚Šã€",
        "äººå·¥çŸ¥èƒ½ã¨è¨€èªå­¦ã®ä¸€åˆ†é‡ã§ã‚ã‚‹ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã®ç ”ç©¶ã¯ã€è¨ˆç®—è¨€èªå­¦ï¼ˆcomputational linguisticsï¼‰ã¨å¯†æ¥ã«é–¢é€£ã—ã¦ãŠã‚Šã€",
        "ä¸¡è€…ã‚’åˆã‚ã›ã¦è¨ˆç®—è¨€èªå­¦ã¨å‘¼ã°ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€éŸ³å£°èªè­˜ã€æ©Ÿæ¢°ç¿»è¨³ã€æƒ…å ±æŠ½å‡ºã€è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã€",
        "ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã€æ„Ÿæƒ…åˆ†æãªã©ã€æ§˜ã€…ãªå¿œç”¨åˆ†é‡ãŒã‚ã‚‹ã€‚",
        "è¿‘å¹´ã§ã¯ã€æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€è‡ªç„¶è¨€èªå‡¦ç†ã®æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã—ã¦ã„ã‚‹ã€‚",
        "ç‰¹ã«ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ãŒå¤šãã®ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚"
    ]
    
    question = "è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿãã®å¿œç”¨åˆ†é‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
    
    print(f"\nğŸ“ Original Text ({len(japanese_context)} sentences):")
    for i, text in enumerate(japanese_context, 1):
        print(f"  {i}. {text}")
    
    print(f"\nâ“ Question: {question}")
    
    # Test different compression rates
    rates = [0.8, 0.6, 0.4, 0.2]
    
    for rate in rates:
        print(f"\nğŸ”§ Compressing with rate {rate}...")
        start_time = time.time()
        
        result = compressor.compress_prompt(
            context=japanese_context,
            question=question,
            rate=rate,
            lang="ja"
        )
        
        end_time = time.time()
        
        print(f"â±ï¸  Compression time: {end_time - start_time:.2f}s")
        print(f"ğŸ“Š Original tokens: {result['origin_tokens']}")
        print(f"ğŸ“Š Compressed tokens: {result['compressed_tokens']}")
        print(f"ğŸ“Š Compression ratio: {result['ratio']}")
        print(f"ğŸ’° Estimated savings: {result['saving']}")
        
        print(f"\nğŸ“„ Compressed text:")
        print(f"  {result['compressed_prompt'][:200]}...")
        print("-" * 50)


def demo_auto_detection():
    """Demonstrate automatic language detection."""
    
    print("\nğŸŒ Language Auto-Detection Demo")
    print("=" * 50)
    
    compressor = PromptCompressor(device_map="cpu")
    
    # Mixed language text
    mixed_context = [
        "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processingï¼‰ã¯ã€",
        "äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "Machine learning and deep learning are used.",
        "éŸ³å£°èªè­˜ã‚„æ©Ÿæ¢°ç¿»è¨³ãªã©ã®å¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚"
    ]
    
    print("ğŸ“ Mixed Language Text:")
    for text in mixed_context:
        print(f"  {text}")
    
    print("\nğŸ” Testing auto-detection...")
    result = compressor.compress_prompt(
        context=mixed_context,
        rate=0.5,
        lang="auto"
    )
    
    print(f"ğŸ“Š Result: {result['compressed_tokens']} tokens (from {result['origin_tokens']})")
    print(f"ğŸ“„ Compressed: {result['compressed_prompt'][:150]}...")


def demo_performance_comparison():
    """Compare performance with and without Japanese processing."""
    
    print("\nâš¡ Performance Comparison")
    print("=" * 50)
    
    compressor = PromptCompressor(device_map="cpu")
    
    # English text
    english_context = [
        "Natural Language Processing (NLP) is a field of artificial intelligence.",
        "It focuses on the interaction between computers and human language.",
        "Machine learning and deep learning techniques are widely used.",
        "Applications include speech recognition, machine translation, and text summarization."
    ]
    
    # Japanese text
    japanese_context = [
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™ã€‚",
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨äººé–“ã®è¨€èªã®ç›¸äº’ä½œç”¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã®æŠ€è¡“ãŒåºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "å¿œç”¨åˆ†é‡ã«ã¯éŸ³å£°èªè­˜ã€æ©Ÿæ¢°ç¿»è¨³ã€ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ãªã©ãŒã‚ã‚Šã¾ã™ã€‚"
    ]
    
    print("ğŸ”¤ English Text Processing:")
    start_time = time.time()
    en_result = compressor.compress_prompt(context=english_context, rate=0.5)
    en_time = time.time() - start_time
    
    print(f"  Time: {en_time:.2f}s")
    print(f"  Tokens: {en_result['compressed_tokens']}/{en_result['origin_tokens']}")
    
    print("\nğŸ‡¯ğŸ‡µ Japanese Text Processing:")
    start_time = time.time()
    jp_result = compressor.compress_prompt(context=japanese_context, rate=0.5, lang="ja")
    jp_time = time.time() - start_time
    
    print(f"  Time: {jp_time:.2f}s")
    print(f"  Tokens: {jp_result['compressed_tokens']}/{jp_result['origin_tokens']}")
    
    print(f"\nğŸ“ˆ Performance difference: {jp_time - en_time:.2f}s")


if __name__ == "__main__":
    try:
        demo_japanese_compression()
        demo_auto_detection()
        demo_performance_comparison()
        
        print("\nâœ… Demo completed successfully!")
        print("\nğŸ’¡ Tips:")
        print("  - Use lang='auto' for automatic language detection")
        print("  - Use lang='ja' for explicit Japanese processing")
        print("  - Adjust rate parameter to control compression level")
        print("  - Japanese tokenization preserves sentence structure")
        
    except Exception as e:
        print(f"âŒ Error during demo: {e}")
        print("Make sure you have installed the required dependencies:")
        print("  pip install fugashi unidic-lite") 